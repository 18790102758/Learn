{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python第三方库概览"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Python第三方库的获取和安装\n",
    "\n",
    "#### pip工具安装\n",
    "\n",
    "最常用且最高效的Python第三方库安装方式是 采用pip工具安装。pip是Python官方提供并维 护的在线第三方库安装工具。\n",
    "\n",
    "pip install <拟安装库名>\n",
    "\n",
    "pip是Python第三方库最主要的安装方式，可以安装超 过90%以上的第三方库。然而，还有一些第三方库无法 暂时用pip安装，此时，需要其他的安装方法。 \n",
    "\n",
    "pip工具与操作系统也有关系，在Mac OS X和Linux等操 作系统中，pip工具几乎可以安装任何Python第三方库， 在Windows操作系统中，有一些第三方库仍然需要用其 他方式尝试安装。\n",
    "\n",
    "#### 自定义安装\n",
    "\n",
    "自定义安装指按照第三方库提供的步骤和方式安装。第三方库都有主页用于维护库的代码和文档。\n",
    "\n",
    "#### 文件安装\n",
    "\n",
    "为了解决这类第三方库安装问题，美国加州大学尔湾分 校提供了一个页面，帮助Python用户获得Windows可直 接安装的第三方库文件。\n",
    "\n",
    "#### Python第三方库的获取和安装\n",
    "\n",
    "对于上述三种安装方式，一般优先选择采用pip 工具安装，如果安装失败，则选择自定义安装或\n",
    "者文件安装。另外，如果需要在没有网络条件下 安装Python第三方库，请直接采用文件安装方 式。其中，.whl文件可以通过pip download指 令在有网络条件的情况下获得。\n",
    "\n",
    "pip支持安装（install）、下载（download）、 卸载（uninstall）、列表（list）、查看（list）、 查找（search）等一系列安装和维护子命令。\n",
    "\n",
    "- pip的uninstall子命令可以卸载一个已经安装的 第三方库，格式如下：\n",
    "\n",
    "pip uninstall <拟卸载库名>\n",
    "\n",
    "- pip的list子命令可以列出当前系统中已经安装的 第三方库，格式如下： \n",
    "\n",
    "pip list\n",
    "\n",
    "- pip的show子命令列出某个已经安装库的详细信 息，格式如下：\n",
    "\n",
    "pip show <拟查询库名> \n",
    "\n",
    "- pip的download子命令可以下载第三方库的安装 包，但并不安装，格式如下：\n",
    "\n",
    "pip download\n",
    "\n",
    "- pip的search子命令可以联网搜索库名或摘要中关键字， 格式如下： \n",
    " \n",
    "pip search <拟查询关键字> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 PyInstaller库概述\n",
    "\n",
    "#### PyInstaller库概述\n",
    "\n",
    "PyInstaller是一个十分有用的Python第三方库，它能够 在Windows、Linux、Mac OS X等操作系统下将Python 源文件打包，变成直接可运行的可执行文件。 \n",
    "\n",
    "通过对源文件打包，Python程序可以在没有安装Python 的环境中运行，也可以作为一个独立文件方便传递和管\n",
    "理。\n",
    "\n",
    "### 3 PyInstaller库与程序打包\n",
    "\n",
    "#### PyInstaller库与程序打包\n",
    "\n",
    "使用PyInstaller库对Python源文件打包十分简单， 使用方法如下： \n",
    "\n",
    ":\\>PyInstaller <Python源程序文件名>\n",
    "\n",
    "执行完毕后，源文件所在目录将生成dist和build 两个文件夹。最终的打包程序在dist内部与源文 件同名的目录中\n",
    "\n",
    "可以通过-F参数对Python源文件生成一个独立的 可执行文件，如下：\n",
    "\n",
    ":\\>PyInstaller -F <Python源程序文件名>\n",
    "\n",
    "执行后在dist目录中出现了SnowView.exe文件， 没有任何依赖库，执行它即可显示雪景效果。\n",
    "\n",
    " PyInstaller有一些常用参数\n",
    "\n",
    "|参数 |功能|\n",
    "|:--:|:--:|\n",
    "|-h, --help| 查看帮助 |\n",
    "|--clean |清理打包过程中的临时文件|\n",
    "|-D, --onedir| 默认值，生成dist目录 |\n",
    "|-F, --onefile| 在dist文件夹中只生成独立的打包文件|\n",
    "|-i < 图 标 文 件 名 .ico >| 指定打包程序使用的图标（icon）文件|\n",
    "\n",
    "### 4 jieba库概述\n",
    "\n",
    "#### jieba库概述\n",
    "\n",
    "由于中文文本中的单词不是通过空格或者标点符号分割，中文及类似语言存在一个重要的“分词”问题。\n",
    "\n",
    "jieba（“结巴”）是Python中一个重要的第三 方中文分词函数库。\n",
    "\n",
    "jieba库的分词原理是利用一个中文词库，将待分 词的内容与分词词库进行比对，通过图结构和动态规划方法找到最大概率的词组。除了分词， jieba还提供增加自定义中文单词的功能。\n",
    "\n",
    "ieba库支持三种分词模式：精确模式，将句子最 精确地切开，适合文本分析；全模式，把句子中\n",
    "所有可以成词的词语都扫描出来，速度非常快，但是不能解决歧义；搜索引擎模式，在精确模式\n",
    "基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。\n",
    "\n",
    "对中文分词来说，jieba库只需要一行代码即可。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\Lenovo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.834 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['全国', '计算机', '等级', '考试']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "jieba.lcut(\"全国计算机等级考试\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 jieba库与中文分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### jieba库与中文分词\n",
    "\n",
    "jieba.lcut(s)是最常用的中文分词函数，用于精准 模式，即将字符串分割成等量的中文词组，返回结果是列表类型。\n",
    "\n",
    "jieba.lcut(s, cut_all = True)用于全模式，即将字 符串的所有分词可能均列出来，返回结果是列表类型，冗余性最大。\n",
    "\n",
    " jieba.lcut_for_search(s)返回搜索引擎模式，该 模式首先执行精确模式，然后再对其中长词进一步切分获得最终结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['全国', '计算机', '等级', '考试', 'Python', '科目']\n",
      "['全国', '国计', '计算', '计算机', '算机', '等级', '考试', 'Python', '科目']\n",
      "['全国', '计算', '算机', '计算机', '等级', '考试', 'Python', '科目']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "ls1 = jieba.lcut(\"全国计算机等级考试Python科目\",)\n",
    "ls2 = jieba.lcut(\"全国计算机等级考试Python科目\", cut_all=True)\n",
    "ls3 = jieba.lcut_for_search(\"全国计算机等级考试Python科目\") \n",
    "\n",
    "print(ls1)\n",
    "print(ls2)\n",
    "print(ls3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搜索引擎模式更倾向于寻找短词语，这种方式具有一定冗余度，但冗余度相比全模式较少。\n",
    "\n",
    "如果希望对文本准确分词，不产生冗余，只能选 择jieba.lcut(s)函数，即精确模式。如果希望对文 本分词更准确，不漏掉任何可能的分词结果，请选用全模式。如果没想好怎么用，可以使用搜索引擎模式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jieba.add_word()函数，顾名思义，用来向jieba 词库增加新的单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['全国', '计算机', '等级', '考试', 'Python科目']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "jieba.add_word(\"Python科目\")\n",
    "ls = jieba.lcut(\"全国计算机等级考试Python科目\") \n",
    "\n",
    "print(ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 wordcloud库概述\n",
    "\n",
    "#### wordcloud库概述\n",
    "\n",
    "词云以词语为基本单元，根据其在文本中出现的频率设计不同大小以形成视觉上不同效果，形成\n",
    "“关键词云层”或“关键词渲染”，从而使读者只要“一瞥”即可领略文本的主旨。\n",
    "\n",
    " wordcloud库的使用十分简单，以一个字符串为 例。其中，产生词云只需要一行语句，在第三行，并可以将词云保存为图片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1ebb0d43710>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wordcloud import WordCloud \n",
    "\n",
    "txt = 'I like python. I am learning python' \n",
    "wordcloud = WordCloud().generate(txt) \n",
    "wordcloud.to_file('testcloud.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 wordcloud库与可视化词云"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### wordcloud库与可视化词云\n",
    "\n",
    "在生成词云时，wordcloud默认会以空格或标点 为分隔符对目标文本进行分词处理。对于中文文\n",
    "本，分词处理需要由用户来完成。一般步骤是先将文本分词处理，然后以空格拼接，再调用 wordcloud库函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1ebb6d0c0b8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "from wordcloud import WordCloud\n",
    "txt = '''程序设计语言是计算机能够理解和识别用户操作意图的一种交互体系，\n",
    "        它按 照特定规则组织计算机指令，使计算机能够自动进行各种运算处理。'''\n",
    "words = jieba.lcut(txt)        # 精确分词 \n",
    "newtxt = ' '.join(words)       # 空格拼接 \n",
    "wordcloud = WordCloud(font_path=\"msyh.ttc\").generate(newtxt) \n",
    "wordcloud.to_file('词云中文例子图.png')# 保存图片 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wordcloud库的核心是WordColoud类，所有的 功能都封装在WordCloud类中。使用时需要实 例化一个WordColoud类的对象，并调用其 generate(text)方法将text文本转化为词云。\n",
    "\n",
    "WordCloud类的常用方法\n",
    "\n",
    "|方法| 功能|\n",
    "|:--:|:--:|\n",
    "|generate(text)| 由text文本生成词云 |\n",
    "|to_file(filename)| 将词云图保存为名为filename的文件|\n",
    "\n",
    "WordCloud对象创建的常用参数\n",
    "\n",
    "|参数|功能|\n",
    "|:--:|:--:|\n",
    "|font_path| 指定字体文件的完整路径，默认None|\n",
    "|width |生成图片宽度，默认400像素|\n",
    "|height| 生成图片高度，默认200像素|\n",
    "|mask |词云形状，默认None，即，方形图 |\n",
    "|min_font_size|词云中最小的字体字号，默认4号 |\n",
    "|font_step |字号步进间隔，默认1 |\n",
    "|manx_font_size| 词云中最大的字体字号，默认None，根据高度自动调节|\n",
    "|max_words| 词云图中最大词数，默认200|\n",
    "|stopwords |被排除词列表，排除词不在词云中显示 |\n",
    "|background_color |图片背景颜色，默认黑色|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'imread' from 'scipy.misc' (E:\\Anaconda\\lib\\site-packages\\scipy\\misc\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-88754ac710f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimread\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'AliceMask.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'imread' from 'scipy.misc' (E:\\Anaconda\\lib\\site-packages\\scipy\\misc\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud \n",
    "from scipy.misc import imread\n",
    " \n",
    "mask = imread('AliceMask.png')\n",
    " \n",
    "with open('AliceInWonderland.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "    wordcloud = WordCloud(background_color=\"white\", \\\n",
    "                          width=800, \\\n",
    "                          height=600, \\\n",
    "                          max_words=200, \\\n",
    "                          max_font_size=80, \\\n",
    "                          mask = mask, \\\n",
    "                         ).generate(text)\n",
    "# 保存图片 \n",
    "wordcloud.to_file('AliceInWonderland.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，from scipy.misc import imread一行用于 将AliceMask.png读取为nd-array类型，用于后 面传递给mask参数使用。（这个库函数隶属于 scipy库，pip在安装wordcloud库时会自动安装 依赖库。）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 实例解析：《三国演义》人物出场词云\n",
    "\n",
    "#### 《三国演义》人物出场统计\n",
    "\n",
    "《三国演义》是一本鸿篇巨著，里面出现了几百个各具特色的人物。每次读这本经典作品都会想一个问题，全书这些人物谁出场最多呢？一起来用 Python回答这个问题吧。人物出场统计涉及对词汇的统计。中文文章需要 分词才能进行词频统计，这需要用到jieba库。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "曹操         1451\n",
      "孔明         1383\n",
      "刘备         1252\n",
      "关羽          784\n",
      "张飞          358\n",
      "吕布          300\n",
      "赵云          278\n",
      "孙权          264\n",
      "司马懿         221\n",
      "周瑜          217\n",
      "袁绍          191\n",
      "马超          185\n",
      "魏延          180\n",
      "黄忠          168\n",
      "姜维          151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1ebb6cfba90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "txt = open(\"threekingdoms.txt\",\"r\",encoding=\"utf-8\").read()\n",
    "excludes = {\"将军\",\"却说\",\"荆州\",\"二人\",\"不可\",\"不能\",\"如此\",\"商议\",\\\n",
    "            \"如何\",\"主公\",\"左右\",\"军马\",\"引兵\",\"次日\",\"大喜\",\"天下\",\\\n",
    "            \"东吴\",\"于是\",\"今日\",\"不敢\",\"魏兵\",\"陛下\",\"一人\",\"都督\",\\\n",
    "            \"人马\",\"军士\",\"汉中\",\"只见\",\"众将\",\"后主\",\"蜀兵\",\"上马\",\\\n",
    "            \"不知\",\"大叫\",\"太守\",\"此人\",\"夫人\",\"先主\",\"后人\",\"背后\",\\\n",
    "            \"城中\",\"天子\",\"一面\",\"何不\",\"大军\",\"忽报\",\"先生\",\"百姓\",\\\n",
    "            \"何故\",\"然后\",\"先锋\",\"不如\",\"赶来\"}\n",
    "words = jieba.lcut(txt)\n",
    "counts = {}\n",
    "for word in words:\n",
    "    if len(word) == 1:\n",
    "        continue\n",
    "    elif word == \"诸葛亮\"or word == \"孔明曰\":\n",
    "            rword = \"孔明\"\n",
    "    elif word == \"关公\"or word == \"云长\":\n",
    "            rword = \"关羽\"\n",
    "    elif word == \"玄德\"or word == \"玄德曰\":\n",
    "            rword = \"刘备\"\n",
    "    elif word == \"孟德\"or word == \"丞相\":\n",
    "            rword = \"曹操\"\n",
    "    else:\n",
    "        rword = word\n",
    "    counts[rword] = counts.get(rword,0) + 1\n",
    "for word in excludes:\n",
    "    del counts[word]\n",
    "items = list(counts.items())\n",
    "items.sort(key=lambda x:x[1],reverse=True)\n",
    "for i in range(15):\n",
    "    word,count = items[i]\n",
    "    print(\"{0:<10}{1:>5}\".format(word,count))\n",
    "words  = jieba.lcut(txt) \n",
    "newtxt = ' '.join(words)\n",
    "wordcloud = WordCloud(background_color=\"white\", \\\n",
    "                      width=800, \\\n",
    "                      height=600, \\\n",
    "                      font_path=\"msyh.ttc\", \\\n",
    "                      max_words=200, \\\n",
    "                      max_font_size=80, \\\n",
    "                      stopwords = excludes, \\\n",
    "                     ).generate(newtxt)\n",
    "wordcloud.to_file('三国演义基本词云.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
